---
phase: 05-ai-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - apps/backend/src/modules/ai/services/rate-limiter.service.ts
  - apps/backend/src/modules/ai/dto/rate-limit.dto.ts
  - apps/backend/prisma/schema.prisma
  - apps/backend/src/modules/ai/ai.module.ts
autonomous: true

must_haves:
  truths:
    - "Each organization has isolated rate limits"
    - "Rate limiter tracks both requests per minute (RPM) and tokens per minute (TPM)"
    - "Rate limit check returns remaining capacity"
    - "Actual token usage is recorded for billing analytics"
  artifacts:
    - path: "apps/backend/src/modules/ai/services/rate-limiter.service.ts"
      provides: "Per-tenant AI rate limiting with Redis"
      exports: ["AiRateLimiterService"]
      min_lines: 100
    - path: "apps/backend/prisma/schema.prisma"
      provides: "AiUsage model for tracking"
      contains: "model AiUsage"
  key_links:
    - from: "apps/backend/src/modules/ai/services/rate-limiter.service.ts"
      to: "ioredis"
      via: "Redis sorted sets"
      pattern: "zadd|zremrangebyscore|zcard"
    - from: "apps/backend/src/modules/ai/services/rate-limiter.service.ts"
      to: "apps/backend/prisma/schema.prisma"
      via: "PrismaService for AiUsage"
      pattern: "prisma\\.aiUsage"
---

<objective>
Implement per-tenant AI rate limiting using Redis sorted sets for accurate sliding window tracking. This prevents any single tenant from exhausting shared API limits and enables usage analytics.

Purpose: Multi-tenant rate limiting is critical for fair resource allocation and preventing cascading failures when one organization has heavy AI usage.

Output: AiRateLimiterService with checkAndConsume() and recordUsage() methods, plus AiUsage model for billing.
</objective>

<execution_context>
@C:\Users\cu0718\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\cu0718\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-ai-infrastructure/05-RESEARCH.md

# Prior phase context
@.planning/phases/01-foundation-infrastructure/01-02-SUMMARY.md (Redis via BullMQ, ioredis)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add AiUsage model to Prisma schema</name>
  <files>
    apps/backend/prisma/schema.prisma
  </files>
  <action>
Add the AiUsage model to track token consumption per organization for billing and analytics.

Add after existing models (near the end of schema):
```prisma
// ===========================================
// AI Usage Tracking
// ===========================================

/// AiUsage tracks token consumption per organization for billing and analytics.
/// Each record represents a single AI API call with its token counts.
model AiUsage {
  id              String   @id @default(uuid())
  organizationId  String   @map("organization_id")
  userId          String?  @map("user_id") // User who triggered the AI call

  // Token counts
  inputTokens     Int      @map("input_tokens")
  outputTokens    Int      @map("output_tokens")
  cacheReadTokens Int      @default(0) @map("cache_read_tokens")
  cacheWriteTokens Int     @default(0) @map("cache_write_tokens")

  // Request metadata
  model           String   // Model used (e.g., 'claude-sonnet-4-5')
  provider        String   @default("claude") // Provider name
  featureType     String?  @map("feature_type") // e.g., 'summary', 'note-cleanup', 'chat'
  entityType      String?  @map("entity_type") // Entity context (case, investigation)
  entityId        String?  @map("entity_id")

  // Timing
  durationMs      Int?     @map("duration_ms") // Request duration in milliseconds
  timestamp       DateTime @default(now())

  @@index([organizationId, timestamp])
  @@index([organizationId, featureType])
  @@index([userId, timestamp])
  @@map("ai_usage")
}

/// AiRateLimit stores per-organization rate limit configuration.
/// Allows customizing limits per customer tier.
model AiRateLimit {
  id               String @id @default(uuid())
  organizationId   String @unique @map("organization_id")

  requestsPerMinute Int    @default(60) @map("requests_per_minute")
  tokensPerMinute   Int    @default(100000) @map("tokens_per_minute")
  requestsPerDay    Int    @default(10000) @map("requests_per_day")
  tokensPerDay      Int    @default(5000000) @map("tokens_per_day")

  createdAt        DateTime @default(now()) @map("created_at")
  updatedAt        DateTime @updatedAt @map("updated_at")

  @@map("ai_rate_limits")
}
```

Run migration:
```bash
cd apps/backend && npx prisma migrate dev --name add_ai_usage_tracking
```
  </action>
  <verify>
Migration succeeds without errors.
`npx prisma generate` completes.
AiUsage and AiRateLimit types appear in generated client.
  </verify>
  <done>
AiUsage and AiRateLimit models exist in schema with proper indexes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create AiRateLimiterService</name>
  <files>
    apps/backend/src/modules/ai/services/rate-limiter.service.ts
    apps/backend/src/modules/ai/dto/rate-limit.dto.ts
  </files>
  <action>
Create the rate limiter service using Redis sorted sets:

1. Create `apps/backend/src/modules/ai/dto/rate-limit.dto.ts`:
```typescript
export interface RateLimitCheckParams {
  organizationId: string;
  estimatedTokens: number;
}

export interface RateLimitResult {
  allowed: boolean;
  reason?: 'RATE_LIMIT_RPM' | 'RATE_LIMIT_TPM' | 'RATE_LIMIT_DAILY';
  retryAfterMs?: number;
  remaining?: {
    rpm: number;
    tpm: number;
    dailyRequests?: number;
    dailyTokens?: number;
  };
}

export interface RecordUsageParams {
  organizationId: string;
  userId?: string;
  inputTokens: number;
  outputTokens: number;
  cacheReadTokens?: number;
  cacheWriteTokens?: number;
  model: string;
  provider?: string;
  featureType?: string;
  entityType?: string;
  entityId?: string;
  durationMs?: number;
}

export interface OrgRateLimits {
  requestsPerMinute: number;
  tokensPerMinute: number;
  requestsPerDay: number;
  tokensPerDay: number;
}
```

2. Create `apps/backend/src/modules/ai/services/rate-limiter.service.ts`:
```typescript
import { Injectable, Logger, Inject } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import Redis from 'ioredis';
import { PrismaService } from '../../prisma/prisma.service';
import {
  RateLimitCheckParams,
  RateLimitResult,
  RecordUsageParams,
  OrgRateLimits,
} from '../dto/rate-limit.dto';

const DEFAULT_LIMITS: OrgRateLimits = {
  requestsPerMinute: 60,
  tokensPerMinute: 100000,
  requestsPerDay: 10000,
  tokensPerDay: 5000000,
};

@Injectable()
export class AiRateLimiterService {
  private readonly logger = new Logger(AiRateLimiterService.name);
  private redis: Redis;
  private readonly limitsCache = new Map<string, { limits: OrgRateLimits; expiresAt: number }>();
  private readonly limitsCacheTtl = 60000; // 1 minute cache

  constructor(
    private readonly configService: ConfigService,
    private readonly prisma: PrismaService,
  ) {
    const redisUrl = this.configService.get<string>('REDIS_URL', 'redis://localhost:6379');
    this.redis = new Redis(redisUrl);
  }

  private getKey(orgId: string, type: 'rpm' | 'tpm' | 'daily-rpm' | 'daily-tpm'): string {
    const today = new Date().toISOString().slice(0, 10); // YYYY-MM-DD
    if (type.startsWith('daily')) {
      return `ai:ratelimit:${orgId}:${type}:${today}`;
    }
    return `ai:ratelimit:${orgId}:${type}`;
  }

  /**
   * Check if request is allowed and consume quota if so.
   * Uses Redis sorted sets for accurate sliding window rate limiting.
   */
  async checkAndConsume(params: RateLimitCheckParams): Promise<RateLimitResult> {
    const { organizationId, estimatedTokens } = params;
    const limits = await this.getOrgLimits(organizationId);
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window

    // Keys
    const rpmKey = this.getKey(organizationId, 'rpm');
    const tpmKey = this.getKey(organizationId, 'tpm');
    const dailyRpmKey = this.getKey(organizationId, 'daily-rpm');
    const dailyTpmKey = this.getKey(organizationId, 'daily-tpm');

    // Use pipeline for atomic operations
    const pipeline = this.redis.pipeline();

    // Clean old entries and get counts for RPM
    pipeline.zremrangebyscore(rpmKey, 0, windowStart);
    pipeline.zcard(rpmKey);

    // Clean old entries and get token sum for TPM
    pipeline.zremrangebyscore(tpmKey, 0, windowStart);
    pipeline.zrange(tpmKey, 0, -1, 'WITHSCORES');

    // Daily counts (simple counters, reset at midnight)
    pipeline.get(dailyRpmKey);
    pipeline.get(dailyTpmKey);

    const results = await pipeline.exec();

    // Parse results
    const currentRpm = (results![1][1] as number) || 0;
    const tpmEntries = (results![3][1] as string[]) || [];
    const currentTpm = this.sumTokensFromEntries(tpmEntries);
    const dailyRpm = parseInt((results![4][1] as string) || '0', 10);
    const dailyTpm = parseInt((results![5][1] as string) || '0', 10);

    // Check RPM limit
    if (currentRpm >= limits.requestsPerMinute) {
      const oldestEntry = await this.getOldestEntry(rpmKey, windowStart);
      const retryAfterMs = oldestEntry ? (oldestEntry + 60000 - now) : 1000;

      return {
        allowed: false,
        reason: 'RATE_LIMIT_RPM',
        retryAfterMs: Math.max(retryAfterMs, 0),
      };
    }

    // Check TPM limit
    if (currentTpm + estimatedTokens > limits.tokensPerMinute) {
      return {
        allowed: false,
        reason: 'RATE_LIMIT_TPM',
        retryAfterMs: this.calculateTpmRetryAfter(tpmEntries, windowStart, now),
      };
    }

    // Check daily limits
    if (dailyRpm >= limits.requestsPerDay) {
      return {
        allowed: false,
        reason: 'RATE_LIMIT_DAILY',
        retryAfterMs: this.msUntilMidnight(),
      };
    }

    if (dailyTpm + estimatedTokens > limits.tokensPerDay) {
      return {
        allowed: false,
        reason: 'RATE_LIMIT_DAILY',
        retryAfterMs: this.msUntilMidnight(),
      };
    }

    // Consume quota
    const requestId = `${now}:${Math.random().toString(36).slice(2, 10)}`;
    const consumePipeline = this.redis.pipeline();

    // Add to RPM counter
    consumePipeline.zadd(rpmKey, now, requestId);
    consumePipeline.expire(rpmKey, 120);

    // Add to TPM counter (store tokens in member name for summing)
    consumePipeline.zadd(tpmKey, now, `${requestId}:${estimatedTokens}`);
    consumePipeline.expire(tpmKey, 120);

    // Increment daily counters
    consumePipeline.incr(dailyRpmKey);
    consumePipeline.expire(dailyRpmKey, 86400 + 3600); // 25 hours to survive timezone edge cases
    consumePipeline.incrby(dailyTpmKey, estimatedTokens);
    consumePipeline.expire(dailyTpmKey, 86400 + 3600);

    await consumePipeline.exec();

    return {
      allowed: true,
      remaining: {
        rpm: limits.requestsPerMinute - currentRpm - 1,
        tpm: limits.tokensPerMinute - currentTpm - estimatedTokens,
        dailyRequests: limits.requestsPerDay - dailyRpm - 1,
        dailyTokens: limits.tokensPerDay - dailyTpm - estimatedTokens,
      },
    };
  }

  /**
   * Record actual token usage after API call completes.
   */
  async recordUsage(params: RecordUsageParams): Promise<void> {
    const totalTokens = params.inputTokens + params.outputTokens;

    // Store in database for billing and analytics
    await this.prisma.aiUsage.create({
      data: {
        organizationId: params.organizationId,
        userId: params.userId,
        inputTokens: params.inputTokens,
        outputTokens: params.outputTokens,
        cacheReadTokens: params.cacheReadTokens || 0,
        cacheWriteTokens: params.cacheWriteTokens || 0,
        model: params.model,
        provider: params.provider || 'claude',
        featureType: params.featureType,
        entityType: params.entityType,
        entityId: params.entityId,
        durationMs: params.durationMs,
      },
    });

    // Update actual TPM if significantly different from estimate
    // This helps maintain accurate rate limiting
    this.logger.debug(
      `AI usage: org=${params.organizationId} tokens=${totalTokens} feature=${params.featureType}`,
    );
  }

  /**
   * Get usage statistics for an organization.
   */
  async getUsageStats(
    organizationId: string,
    period: 'day' | 'week' | 'month' = 'day',
  ): Promise<{
    totalRequests: number;
    totalInputTokens: number;
    totalOutputTokens: number;
    byFeature: Record<string, { requests: number; tokens: number }>;
  }> {
    const periodStart = new Date();
    if (period === 'day') {
      periodStart.setHours(0, 0, 0, 0);
    } else if (period === 'week') {
      periodStart.setDate(periodStart.getDate() - 7);
    } else {
      periodStart.setDate(periodStart.getDate() - 30);
    }

    const usage = await this.prisma.aiUsage.findMany({
      where: {
        organizationId,
        timestamp: { gte: periodStart },
      },
    });

    const byFeature: Record<string, { requests: number; tokens: number }> = {};

    let totalInputTokens = 0;
    let totalOutputTokens = 0;

    for (const record of usage) {
      totalInputTokens += record.inputTokens;
      totalOutputTokens += record.outputTokens;

      const feature = record.featureType || 'unknown';
      if (!byFeature[feature]) {
        byFeature[feature] = { requests: 0, tokens: 0 };
      }
      byFeature[feature].requests++;
      byFeature[feature].tokens += record.inputTokens + record.outputTokens;
    }

    return {
      totalRequests: usage.length,
      totalInputTokens,
      totalOutputTokens,
      byFeature,
    };
  }

  /**
   * Get rate limits for an organization.
   * Caches results for performance.
   */
  private async getOrgLimits(organizationId: string): Promise<OrgRateLimits> {
    // Check cache
    const cached = this.limitsCache.get(organizationId);
    if (cached && cached.expiresAt > Date.now()) {
      return cached.limits;
    }

    // Query database
    const config = await this.prisma.aiRateLimit.findUnique({
      where: { organizationId },
    });

    const limits: OrgRateLimits = config
      ? {
          requestsPerMinute: config.requestsPerMinute,
          tokensPerMinute: config.tokensPerMinute,
          requestsPerDay: config.requestsPerDay,
          tokensPerDay: config.tokensPerDay,
        }
      : DEFAULT_LIMITS;

    // Cache
    this.limitsCache.set(organizationId, {
      limits,
      expiresAt: Date.now() + this.limitsCacheTtl,
    });

    return limits;
  }

  private sumTokensFromEntries(entries: string[]): number {
    let sum = 0;
    // Entries are [member, score, member, score, ...]
    // Member format: requestId:tokenCount
    for (let i = 0; i < entries.length; i += 2) {
      const member = entries[i];
      const parts = member.split(':');
      if (parts.length >= 3) {
        sum += parseInt(parts[2], 10) || 0;
      }
    }
    return sum;
  }

  private async getOldestEntry(key: string, windowStart: number): Promise<number | null> {
    const oldest = await this.redis.zrangebyscore(key, windowStart, '+inf', 'LIMIT', 0, 1);
    if (oldest.length > 0) {
      const parts = oldest[0].split(':');
      return parseInt(parts[0], 10);
    }
    return null;
  }

  private calculateTpmRetryAfter(entries: string[], windowStart: number, now: number): number {
    // Find oldest entry to know when capacity will free up
    for (let i = 0; i < entries.length; i += 2) {
      const member = entries[i];
      const score = parseFloat(entries[i + 1]);
      if (score >= windowStart) {
        return Math.max((score + 60000 - now), 1000);
      }
    }
    return 1000;
  }

  private msUntilMidnight(): number {
    const now = new Date();
    const midnight = new Date(now);
    midnight.setHours(24, 0, 0, 0);
    return midnight.getTime() - now.getTime();
  }
}
```
  </action>
  <verify>
`npm run build` succeeds.
AiRateLimiterService exports checkAndConsume and recordUsage methods.
  </verify>
  <done>
Rate limiter service tracks RPM, TPM, and daily limits per organization using Redis.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire up rate limiter in AI module</name>
  <files>
    apps/backend/src/modules/ai/ai.module.ts
    apps/backend/src/modules/ai/dto/index.ts
    apps/backend/src/modules/ai/index.ts
  </files>
  <action>
1. Update `apps/backend/src/modules/ai/dto/index.ts`:
```typescript
export * from './chat-message.dto';
export * from './rate-limit.dto';
```

2. Update `apps/backend/src/modules/ai/ai.module.ts`:
```typescript
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';
import { PrismaModule } from '../prisma/prisma.module';
import { AiClientService } from './services/ai-client.service';
import { ProviderRegistryService } from './services/provider-registry.service';
import { AiRateLimiterService } from './services/rate-limiter.service';
import { ClaudeProvider } from './providers/claude.provider';

@Module({
  imports: [ConfigModule, PrismaModule],
  providers: [
    ClaudeProvider,
    ProviderRegistryService,
    AiClientService,
    AiRateLimiterService,
  ],
  exports: [
    AiClientService,
    ProviderRegistryService,
    AiRateLimiterService,
    ClaudeProvider,
  ],
})
export class AiModule {}
```

3. Update `apps/backend/src/modules/ai/index.ts`:
```typescript
export * from './ai.module';
export * from './services/ai-client.service';
export * from './services/provider-registry.service';
export * from './services/rate-limiter.service';
export * from './providers';
export * from './interfaces';
export * from './dto';
```

4. Add environment variables to `.env.example`:
```
# AI Rate Limiting (per organization)
AI_DEFAULT_RPM=60
AI_DEFAULT_TPM=100000
AI_DEFAULT_DAILY_REQUESTS=10000
AI_DEFAULT_DAILY_TOKENS=5000000
```
  </action>
  <verify>
`npm run build` succeeds.
AiRateLimiterService is exported from AiModule.
  </verify>
  <done>
Rate limiter integrated into AI module and ready for use by AI features.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. `npm run build` passes
2. `npx prisma migrate status` shows all migrations applied
3. AiUsage and AiRateLimit models exist
4. AiRateLimiterService has checkAndConsume, recordUsage, getUsageStats methods
5. Redis keys use proper namespacing per organization
</verification>

<success_criteria>
- Per-tenant rate limits using Redis sorted sets
- Sliding window algorithm for accurate RPM/TPM tracking
- Daily limits tracked separately
- Usage recorded to database for billing analytics
- Organization-specific limit configuration supported
- Graceful handling when limits exceeded (returns retry-after)
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-infrastructure/05-03-SUMMARY.md`
</output>
