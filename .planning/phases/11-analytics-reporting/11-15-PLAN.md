---
phase: 11-analytics-reporting
plan: 15
type: execute
wave: 3
depends_on: ["11-04"]
files_modified:
  - apps/backend/src/modules/analytics/migration/migration-upload.service.ts
  - apps/backend/src/modules/analytics/migration/migration-upload.controller.ts
  - apps/backend/src/modules/analytics/migration/dto/upload.dto.ts
autonomous: true

must_haves:
  truths:
    - "Migration files can be uploaded to Azure Blob Storage"
    - "File format is auto-detected from content"
    - "Large files support streaming upload"
  artifacts:
    - path: "apps/backend/src/modules/analytics/migration/migration-upload.service.ts"
      provides: "File upload and format detection"
      min_lines: 100
    - path: "apps/backend/src/modules/analytics/migration/migration-upload.controller.ts"
      provides: "Upload endpoint with multipart handling"
      min_lines: 60
  key_links:
    - from: "migration-upload.service.ts"
      to: "storage.service"
      via: "blob storage"
      pattern: "storageService\\."
---

<objective>
Build migration file upload service with format auto-detection.

Purpose: Enable users to upload migration files from competitor systems (MIG-01, MIG-02).
Output: MigrationUploadService with streaming upload and format detection
</objective>

<execution_context>
@C:\Users\cu0718\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\cu0718\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/11-analytics-reporting/11-CONTEXT.md
@.planning/phases/11-analytics-reporting/11-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create upload DTOs</name>
  <files>apps/backend/src/modules/analytics/migration/dto/upload.dto.ts</files>
  <action>
Create upload DTOs:

```typescript
import { IsString, IsOptional, IsEnum, IsInt, Min } from 'class-validator';
import { MigrationSourceType } from '@prisma/client';

export class UploadMigrationFileDto {
  @IsOptional()
  @IsEnum(MigrationSourceType)
  hintSourceType?: MigrationSourceType;

  @IsOptional()
  @IsString()
  description?: string;
}

export class UploadResultDto {
  jobId: string;
  fileName: string;
  fileUrl: string;
  fileSizeBytes: number;
  detectedSourceType: MigrationSourceType;
  detectedFields: string[];
  sampleRows: Record<string, unknown>[];
  confidence: number;
  warnings: string[];
}

export class FormatDetectionResult {
  sourceType: MigrationSourceType;
  confidence: number;        // 0-100
  detectedFields: string[];
  sampleRows: Record<string, unknown>[];
  warnings: string[];
  delimiter?: string;        // For CSV files
  hasHeaders: boolean;
  encoding: string;
  totalRows?: number;
}

// Known field patterns for each source type
export const SOURCE_FIELD_PATTERNS: Record<MigrationSourceType, string[]> = {
  NAVEX: [
    'case_number', 'case_id', 'incident_type', 'incident_date',
    'reporter_type', 'location', 'business_unit', 'subject_name',
    'allegation', 'status', 'assigned_to', 'created_date', 'closed_date',
  ],
  EQS: [
    'report_id', 'report_date', 'category', 'subcategory',
    'description', 'reporter_relationship', 'anonymous',
    'location_country', 'location_region', 'status', 'handler',
  ],
  LEGACY_ETHICO: [
    'call_id', 'call_date', 'hotline_type', 'caller_type',
    'incident_summary', 'organization', 'department',
    'case_status', 'investigator', 'resolution',
  ],
  GENERIC_CSV: [], // Detected dynamically
  ONETRUST: [
    'incident_id', 'created_at', 'type', 'severity',
    'department', 'submitted_by', 'description', 'state',
  ],
  STAR: [
    'matter_id', 'matter_type', 'report_date', 'category',
    'business_area', 'reporter', 'narrative', 'status',
  ],
};
```
  </action>
  <verify>TypeScript compiles without errors</verify>
  <done>Upload DTOs with source field patterns created</done>
</task>

<task type="auto">
  <name>Task 2: Implement MigrationUploadService</name>
  <files>apps/backend/src/modules/analytics/migration/migration-upload.service.ts</files>
  <action>
Create upload service with format detection:

```typescript
import { Injectable, BadRequestException, Logger } from '@nestjs/common';
import { PrismaService } from '../../../common/prisma/prisma.service';
import { StorageService } from '../../storage/storage.service';
import { MigrationSourceType } from '@prisma/client';
import { parse as csvParse } from 'csv-parse';
import { Readable } from 'stream';
import * as xlsx from 'xlsx';
import { nanoid } from 'nanoid';
import {
  FormatDetectionResult,
  SOURCE_FIELD_PATTERNS,
  UploadResultDto,
} from './dto/upload.dto';

const MAX_FILE_SIZE = 100 * 1024 * 1024; // 100MB
const SAMPLE_ROWS = 10;

@Injectable()
export class MigrationUploadService {
  private readonly logger = new Logger(MigrationUploadService.name);

  constructor(
    private prisma: PrismaService,
    private storageService: StorageService,
  ) {}

  /**
   * Upload migration file and create job.
   */
  async uploadFile(
    orgId: string,
    userId: string,
    file: Express.Multer.File,
    hintSourceType?: MigrationSourceType,
  ): Promise<UploadResultDto> {
    // Validate file
    if (!file) {
      throw new BadRequestException('No file provided');
    }
    if (file.size > MAX_FILE_SIZE) {
      throw new BadRequestException(`File too large. Maximum size is ${MAX_FILE_SIZE / 1024 / 1024}MB`);
    }

    const extension = file.originalname.split('.').pop()?.toLowerCase();
    if (!['csv', 'xlsx', 'xls'].includes(extension || '')) {
      throw new BadRequestException('Invalid file type. Supported: CSV, XLSX, XLS');
    }

    // Upload to blob storage
    const blobKey = `migrations/${orgId}/${nanoid()}-${file.originalname}`;
    const fileUrl = await this.storageService.upload(blobKey, file.buffer, file.mimetype);

    // Detect format
    const detection = await this.detectFormat(file.buffer, extension!, hintSourceType);

    // Create migration job
    const job = await this.prisma.migrationJob.create({
      data: {
        organizationId: orgId,
        sourceType: detection.sourceType,
        fileName: file.originalname,
        fileUrl,
        fileSizeBytes: file.size,
        status: 'PENDING',
        totalRows: detection.totalRows,
        createdById: userId,
      },
    });

    this.logger.log(
      `Migration file uploaded: ${file.originalname} (${detection.sourceType}) - Job ${job.id}`,
    );

    return {
      jobId: job.id,
      fileName: file.originalname,
      fileUrl,
      fileSizeBytes: file.size,
      detectedSourceType: detection.sourceType,
      detectedFields: detection.detectedFields,
      sampleRows: detection.sampleRows,
      confidence: detection.confidence,
      warnings: detection.warnings,
    };
  }

  /**
   * Detect file format and source type.
   */
  async detectFormat(
    buffer: Buffer,
    extension: string,
    hintSourceType?: MigrationSourceType,
  ): Promise<FormatDetectionResult> {
    let rows: Record<string, unknown>[];
    let headers: string[];
    let totalRows: number;
    let delimiter = ',';

    if (extension === 'csv') {
      const { parsedRows, detectedDelimiter, detectedHeaders, rowCount } =
        await this.parseCsv(buffer);
      rows = parsedRows;
      headers = detectedHeaders;
      totalRows = rowCount;
      delimiter = detectedDelimiter;
    } else {
      const { parsedRows, detectedHeaders, rowCount } = this.parseExcel(buffer);
      rows = parsedRows;
      headers = detectedHeaders;
      totalRows = rowCount;
    }

    // Detect source type from headers
    const { sourceType, confidence } = this.detectSourceType(headers, hintSourceType);

    const warnings: string[] = [];
    if (confidence < 50) {
      warnings.push('Low confidence in format detection. Please verify field mappings carefully.');
    }
    if (totalRows > 10000) {
      warnings.push(`Large file detected (${totalRows.toLocaleString()} rows). Import may take several minutes.`);
    }

    return {
      sourceType,
      confidence,
      detectedFields: headers,
      sampleRows: rows.slice(0, SAMPLE_ROWS),
      warnings,
      delimiter,
      hasHeaders: true,
      encoding: 'utf-8',
      totalRows,
    };
  }

  /**
   * Parse CSV file with delimiter detection.
   */
  private async parseCsv(buffer: Buffer): Promise<{
    parsedRows: Record<string, unknown>[];
    detectedDelimiter: string;
    detectedHeaders: string[];
    rowCount: number;
  }> {
    // Detect delimiter
    const firstLine = buffer.toString('utf-8').split('\n')[0];
    const delimiters = [',', ';', '\t', '|'];
    let bestDelimiter = ',';
    let maxCount = 0;

    for (const d of delimiters) {
      const count = (firstLine.match(new RegExp(d, 'g')) || []).length;
      if (count > maxCount) {
        maxCount = count;
        bestDelimiter = d;
      }
    }

    return new Promise((resolve, reject) => {
      const rows: Record<string, unknown>[] = [];
      let headers: string[] = [];
      let rowCount = 0;

      const parser = csvParse({
        delimiter: bestDelimiter,
        columns: true,
        skip_empty_lines: true,
        trim: true,
        relax_column_count: true,
      });

      parser.on('readable', () => {
        let record;
        while ((record = parser.read()) !== null) {
          if (headers.length === 0) {
            headers = Object.keys(record);
          }
          rows.push(record);
          rowCount++;
        }
      });

      parser.on('error', reject);
      parser.on('end', () => {
        resolve({
          parsedRows: rows,
          detectedDelimiter: bestDelimiter,
          detectedHeaders: headers,
          rowCount,
        });
      });

      Readable.from(buffer).pipe(parser);
    });
  }

  /**
   * Parse Excel file.
   */
  private parseExcel(buffer: Buffer): {
    parsedRows: Record<string, unknown>[];
    detectedHeaders: string[];
    rowCount: number;
  } {
    const workbook = xlsx.read(buffer, { type: 'buffer' });
    const sheetName = workbook.SheetNames[0];
    const sheet = workbook.Sheets[sheetName];

    const rows = xlsx.utils.sheet_to_json(sheet, { defval: '' }) as Record<string, unknown>[];
    const headers = rows.length > 0 ? Object.keys(rows[0]) : [];

    return {
      parsedRows: rows,
      detectedHeaders: headers,
      rowCount: rows.length,
    };
  }

  /**
   * Detect source type from field names.
   */
  private detectSourceType(
    headers: string[],
    hintSourceType?: MigrationSourceType,
  ): { sourceType: MigrationSourceType; confidence: number } {
    // If hint provided and matches well, use it
    if (hintSourceType && hintSourceType !== 'GENERIC_CSV') {
      const patternMatch = this.calculatePatternMatch(headers, hintSourceType);
      if (patternMatch > 30) {
        return { sourceType: hintSourceType, confidence: patternMatch };
      }
    }

    // Try each source type
    const scores: { type: MigrationSourceType; score: number }[] = [];

    for (const [type, patterns] of Object.entries(SOURCE_FIELD_PATTERNS)) {
      if (type === 'GENERIC_CSV') continue;
      const score = this.calculatePatternMatch(headers, type as MigrationSourceType);
      scores.push({ type: type as MigrationSourceType, score });
    }

    // Sort by score descending
    scores.sort((a, b) => b.score - a.score);

    const best = scores[0];
    if (best && best.score >= 30) {
      return { sourceType: best.type, confidence: best.score };
    }

    // Default to generic CSV
    return { sourceType: 'GENERIC_CSV', confidence: 100 };
  }

  /**
   * Calculate match percentage between headers and known patterns.
   */
  private calculatePatternMatch(headers: string[], sourceType: MigrationSourceType): number {
    const patterns = SOURCE_FIELD_PATTERNS[sourceType];
    if (patterns.length === 0) return 0;

    const normalizedHeaders = headers.map(h =>
      h.toLowerCase().replace(/[^a-z0-9]/g, '_'),
    );

    let matchCount = 0;
    for (const pattern of patterns) {
      const normalizedPattern = pattern.toLowerCase().replace(/[^a-z0-9]/g, '_');
      if (normalizedHeaders.some(h => h.includes(normalizedPattern) || normalizedPattern.includes(h))) {
        matchCount++;
      }
    }

    return Math.round((matchCount / patterns.length) * 100);
  }

  /**
   * Get sample data from uploaded file.
   */
  async getSampleData(
    orgId: string,
    jobId: string,
    limit: number = 100,
  ): Promise<Record<string, unknown>[]> {
    const job = await this.prisma.migrationJob.findFirst({
      where: { id: jobId, organizationId: orgId },
    });

    if (!job) {
      throw new BadRequestException('Migration job not found');
    }

    const buffer = await this.storageService.download(job.fileUrl);
    const extension = job.fileName.split('.').pop()?.toLowerCase();

    if (extension === 'csv') {
      const { parsedRows } = await this.parseCsv(buffer);
      return parsedRows.slice(0, limit);
    } else {
      const { parsedRows } = this.parseExcel(buffer);
      return parsedRows.slice(0, limit);
    }
  }
}
```
  </action>
  <verify>npm run lint passes for the service file</verify>
  <done>MigrationUploadService with format detection implemented</done>
</task>

<task type="auto">
  <name>Task 3: Implement upload controller</name>
  <files>apps/backend/src/modules/analytics/migration/migration-upload.controller.ts</files>
  <action>
Create upload controller with multipart handling:

```typescript
import {
  Controller,
  Post,
  Get,
  Param,
  Query,
  UseGuards,
  UseInterceptors,
  UploadedFile,
  Body,
  ParseFilePipe,
  MaxFileSizeValidator,
  FileTypeValidator,
} from '@nestjs/common';
import { FileInterceptor } from '@nestjs/platform-express';
import { JwtAuthGuard } from '../../auth/guards/jwt-auth.guard';
import { RolesGuard } from '../../auth/guards/roles.guard';
import { Roles } from '../../auth/decorators/roles.decorator';
import { TenantId } from '../../auth/decorators/tenant.decorator';
import { CurrentUser } from '../../auth/decorators/current-user.decorator';
import { MigrationUploadService } from './migration-upload.service';
import { UploadMigrationFileDto, UploadResultDto } from './dto/upload.dto';
import { MigrationSourceType } from '@prisma/client';

const MAX_FILE_SIZE = 100 * 1024 * 1024; // 100MB

@Controller('api/v1/migrations/upload')
@UseGuards(JwtAuthGuard, RolesGuard)
export class MigrationUploadController {
  constructor(private uploadService: MigrationUploadService) {}

  /**
   * Upload migration file.
   */
  @Post()
  @Roles('SYSTEM_ADMIN', 'COMPLIANCE_OFFICER')
  @UseInterceptors(FileInterceptor('file'))
  async uploadFile(
    @TenantId() orgId: string,
    @CurrentUser('id') userId: string,
    @UploadedFile(
      new ParseFilePipe({
        validators: [
          new MaxFileSizeValidator({ maxSize: MAX_FILE_SIZE }),
          new FileTypeValidator({
            fileType: /(csv|xlsx|xls|application\/vnd\.openxmlformats-officedocument\.spreadsheetml\.sheet|application\/vnd\.ms-excel|text\/csv)/,
          }),
        ],
      }),
    )
    file: Express.Multer.File,
    @Body() dto: UploadMigrationFileDto,
  ): Promise<UploadResultDto> {
    return this.uploadService.uploadFile(
      orgId,
      userId,
      file,
      dto.hintSourceType,
    );
  }

  /**
   * Get sample data from uploaded file.
   */
  @Get(':jobId/sample')
  @Roles('SYSTEM_ADMIN', 'COMPLIANCE_OFFICER')
  async getSampleData(
    @TenantId() orgId: string,
    @Param('jobId') jobId: string,
    @Query('limit') limit?: string,
  ): Promise<Record<string, unknown>[]> {
    return this.uploadService.getSampleData(
      orgId,
      jobId,
      limit ? parseInt(limit, 10) : 100,
    );
  }

  /**
   * Re-detect format with different source type hint.
   */
  @Post(':jobId/redetect')
  @Roles('SYSTEM_ADMIN', 'COMPLIANCE_OFFICER')
  async redetectFormat(
    @TenantId() orgId: string,
    @Param('jobId') jobId: string,
    @Body('sourceType') sourceType: MigrationSourceType,
  ): Promise<{
    sourceType: MigrationSourceType;
    confidence: number;
    suggestedMappings: { sourceField: string; targetField: string }[];
  }> {
    const job = await this.uploadService['prisma'].migrationJob.findFirst({
      where: { id: jobId, organizationId: orgId },
    });

    if (!job) {
      throw new Error('Migration job not found');
    }

    // Update job with new source type
    await this.uploadService['prisma'].migrationJob.update({
      where: { id: jobId },
      data: { sourceType },
    });

    // Get suggested mappings for new type
    const sample = await this.uploadService.getSampleData(orgId, jobId, 1);
    const fields = sample.length > 0 ? Object.keys(sample[0]) : [];

    // Simple mapping suggestions based on field names
    const suggestedMappings = fields.map(sourceField => ({
      sourceField,
      targetField: this.suggestTargetField(sourceField, sourceType),
    }));

    return {
      sourceType,
      confidence: 100, // User-selected
      suggestedMappings,
    };
  }

  private suggestTargetField(sourceField: string, sourceType: MigrationSourceType): string {
    const normalized = sourceField.toLowerCase().replace(/[^a-z0-9]/g, '_');

    // Common mappings
    const mappings: Record<string, string> = {
      case_number: 'referenceNumber',
      case_id: 'referenceNumber',
      report_id: 'referenceNumber',
      incident_type: 'categoryName',
      category: 'categoryName',
      incident_date: 'incidentDate',
      report_date: 'createdAt',
      created_date: 'createdAt',
      description: 'details',
      allegation: 'details',
      narrative: 'details',
      status: 'status',
      assigned_to: 'assignedToEmail',
      handler: 'assignedToEmail',
      investigator: 'assignedToEmail',
      reporter_type: 'reporterType',
      anonymous: 'isAnonymous',
      location: 'locationName',
      business_unit: 'businessUnitName',
      department: 'businessUnitName',
    };

    for (const [pattern, target] of Object.entries(mappings)) {
      if (normalized.includes(pattern)) {
        return target;
      }
    }

    return ''; // Unmapped
  }
}
```
  </action>
  <verify>npm run lint && npm run typecheck passes</verify>
  <done>MigrationUploadController with multipart upload implemented</done>
</task>

</tasks>

<verification>
```bash
cd apps/backend
npm run lint -- --fix
npm run typecheck
```
</verification>

<success_criteria>
- File upload supports CSV, XLSX, and XLS formats up to 100MB
- Format detection identifies NAVEX, EQS, Legacy Ethico with >50% confidence
- Unknown formats default to GENERIC_CSV
- Sample data available for preview before mapping
- Upload creates MigrationJob in PENDING status
</success_criteria>

<output>
After completion, create `.planning/phases/11-analytics-reporting/11-15-SUMMARY.md`
</output>
