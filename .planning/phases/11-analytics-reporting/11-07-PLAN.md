---
phase: 11-analytics-reporting
plan: 07
type: execute
wave: 2
depends_on: ["11-03"]
files_modified:
  - apps/backend/src/modules/analytics/exports/excel-export.service.ts
  - apps/backend/src/modules/analytics/exports/processors/flat-export.processor.ts
  - apps/backend/src/modules/analytics/exports/exports.controller.ts
autonomous: true

must_haves:
  truths:
    - "Excel exports use streaming for large datasets (10k+ rows)"
    - "Export jobs execute asynchronously with progress tracking"
    - "Export files are stored in Azure Blob with 7-day expiration"
  artifacts:
    - path: "apps/backend/src/modules/analytics/exports/excel-export.service.ts"
      provides: "ExcelJS streaming export"
      min_lines: 100
    - path: "apps/backend/src/modules/analytics/exports/processors/flat-export.processor.ts"
      provides: "BullMQ processor for async exports"
      min_lines: 80
  key_links:
    - from: "flat-export.processor.ts"
      to: "flat-file.service"
      via: "column configuration"
      pattern: "flatFileService\\."
---

<objective>
Build the Excel streaming export service and async export processor.

Purpose: Enable large-scale data exports without memory exhaustion (ANAL-05).
Output: ExcelExportService with streaming and FlatExportProcessor for async jobs
</objective>

<execution_context>
@C:\Users\cu0718\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\cu0718\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/11-analytics-reporting/11-CONTEXT.md
@.planning/phases/11-analytics-reporting/11-RESEARCH.md
@.planning/phases/11-analytics-reporting/11-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ExcelExportService with streaming</name>
  <files>apps/backend/src/modules/analytics/exports/excel-export.service.ts</files>
  <action>
Create ExcelExportService using ExcelJS streaming:

```typescript
import * as ExcelJS from 'exceljs';
import { PassThrough, Readable } from 'stream';
import { Injectable, Logger } from '@nestjs/common';

@Injectable()
export class ExcelExportService {
  private readonly logger = new Logger(ExcelExportService.name);
  private readonly BATCH_SIZE = 1000;

  /**
   * Stream large exports to avoid memory issues.
   * Use for exports >10k rows.
   */
  async streamExport(
    dataSource: AsyncIterable<Record<string, unknown>>,
    columns: ColumnDefinition[],
  ): Promise<PassThrough> {
    const stream = new PassThrough();

    const workbook = new ExcelJS.stream.xlsx.WorkbookWriter({
      stream,
      useStyles: true,
      useSharedStrings: false, // Faster for large files
    });

    const worksheet = workbook.addWorksheet('Export');

    // Set columns with proper widths
    worksheet.columns = columns.map(col => ({
      header: col.label,
      key: col.field,
      width: this.getColumnWidth(col),
      style: this.getColumnStyle(col),
    }));

    // Style header row
    worksheet.getRow(1).font = { bold: true };
    worksheet.getRow(1).fill = {
      type: 'pattern',
      pattern: 'solid',
      fgColor: { argb: 'FFE5E7EB' },
    };
    worksheet.getRow(1).commit();

    // Enable auto-filter
    worksheet.autoFilter = {
      from: { row: 1, column: 1 },
      to: { row: 1, column: columns.length },
    };

    // Freeze header row
    worksheet.views = [{ state: 'frozen', ySplit: 1 }];

    // Stream rows
    let rowCount = 0;
    for await (const row of dataSource) {
      const values = columns.map(col => this.formatCellValue(row[col.field], col));
      worksheet.addRow(values).commit(); // commit() is critical for streaming!
      rowCount++;

      // Yield to event loop every batch
      if (rowCount % this.BATCH_SIZE === 0) {
        await new Promise(resolve => setImmediate(resolve));
      }
    }

    await worksheet.commit();
    await workbook.commit();

    this.logger.log(`Streamed ${rowCount} rows to Excel`);
    return stream;
  }

  /**
   * Generate Excel buffer for smaller exports (<10k rows).
   * Faster for small datasets, includes more formatting.
   */
  async generateBuffer(
    rows: Record<string, unknown>[],
    columns: ColumnDefinition[],
    options?: ExcelOptions,
  ): Promise<Buffer> {
    const workbook = new ExcelJS.Workbook();
    workbook.creator = 'Ethico Risk Intelligence Platform';
    workbook.created = new Date();

    const worksheet = workbook.addWorksheet(options?.sheetName || 'Export');

    // Set columns
    worksheet.columns = columns.map(col => ({
      header: col.label,
      key: col.field,
      width: this.getColumnWidth(col),
    }));

    // Add rows
    rows.forEach(row => {
      const values = columns.map(col => this.formatCellValue(row[col.field], col));
      worksheet.addRow(values);
    });

    // Style header
    const headerRow = worksheet.getRow(1);
    headerRow.font = { bold: true, size: 11 };
    headerRow.fill = {
      type: 'pattern',
      pattern: 'solid',
      fgColor: { argb: 'FFE5E7EB' },
    };
    headerRow.border = {
      bottom: { style: 'thin', color: { argb: 'FFD1D5DB' } },
    };

    // Auto-filter and freeze
    worksheet.autoFilter = { from: 'A1', to: `${String.fromCharCode(64 + columns.length)}1` };
    worksheet.views = [{ state: 'frozen', ySplit: 1 }];

    // Apply conditional formatting for status columns
    columns.forEach((col, idx) => {
      if (col.type === 'status') {
        this.applyStatusFormatting(worksheet, idx + 1, rows.length + 1);
      }
    });

    return workbook.xlsx.writeBuffer() as Promise<Buffer>;
  }

  private getColumnWidth(col: ColumnDefinition): number {
    if (col.width) return col.width;
    switch (col.type) {
      case 'date': return 12;
      case 'datetime': return 18;
      case 'currency': return 15;
      case 'uuid': return 36;
      case 'email': return 25;
      default: return Math.min(50, Math.max(10, col.label.length + 5));
    }
  }

  private getColumnStyle(col: ColumnDefinition): Partial<ExcelJS.Style> {
    switch (col.type) {
      case 'date': return { numFmt: 'YYYY-MM-DD' };
      case 'datetime': return { numFmt: 'YYYY-MM-DD HH:MM' };
      case 'currency': return { numFmt: '"$"#,##0.00' };
      case 'percentage': return { numFmt: '0.0%' };
      default: return {};
    }
  }

  private formatCellValue(value: unknown, col: ColumnDefinition): unknown {
    if (value === null || value === undefined) return '';

    switch (col.type) {
      case 'date':
      case 'datetime':
        return value instanceof Date ? value : new Date(value as string);
      case 'boolean':
        return value ? 'Yes' : 'No';
      case 'currency':
        return typeof value === 'number' ? value : parseFloat(String(value)) || 0;
      default:
        return String(value);
    }
  }

  private applyStatusFormatting(worksheet: ExcelJS.Worksheet, col: number, maxRow: number): void {
    worksheet.addConditionalFormatting({
      ref: `${String.fromCharCode(64 + col)}2:${String.fromCharCode(64 + col)}${maxRow}`,
      rules: [
        { type: 'containsText', operator: 'containsText', text: 'OPEN', style: { fill: { type: 'pattern', pattern: 'solid', bgColor: { argb: 'FFFEF3C7' } } } },
        { type: 'containsText', operator: 'containsText', text: 'CLOSED', style: { fill: { type: 'pattern', pattern: 'solid', bgColor: { argb: 'FFD1FAE5' } } } },
      ],
    });
  }
}

export interface ColumnDefinition {
  field: string;
  label: string;
  type: 'string' | 'number' | 'date' | 'datetime' | 'boolean' | 'currency' | 'percentage' | 'status' | 'uuid' | 'email';
  width?: number;
}

export interface ExcelOptions {
  sheetName?: string;
  includeFormulas?: boolean;
}
```
  </action>
  <verify>npm run lint passes for the service file</verify>
  <done>ExcelExportService supports both streaming (large) and buffer (small) exports with formatting</done>
</task>

<task type="auto">
  <name>Task 2: Implement FlatExportProcessor</name>
  <files>apps/backend/src/modules/analytics/exports/processors/flat-export.processor.ts</files>
  <action>
Create BullMQ processor for async exports:

```typescript
import { Processor, WorkerHost, OnWorkerEvent } from '@nestjs/bullmq';
import { Job } from 'bullmq';
import { Injectable, Logger } from '@nestjs/common';

export const EXPORT_QUEUE_NAME = 'exports';

interface ExportJobData {
  executionId: string;
  organizationId: string;
  exportType: ExportType;
  format: ExportFormat;
  filters: Record<string, unknown>;
  columnConfig: {
    includeInvestigations: boolean;
    maxInvestigations: number;
    includeTaggedFields: boolean;
    includeOverflow: boolean;
  };
  createdById: string;
}

@Processor(EXPORT_QUEUE_NAME, { concurrency: 2 })
@Injectable()
export class FlatExportProcessor extends WorkerHost {
  private readonly logger = new Logger(FlatExportProcessor.name);

  constructor(
    private prisma: PrismaService,
    private flatFileService: FlatFileService,
    private excelExportService: ExcelExportService,
    private storage: StorageService,
  ) {
    super();
  }

  async process(job: Job<ExportJobData>): Promise<{ fileKey: string; rowCount: number }> {
    const { executionId, organizationId, exportType, format, filters, columnConfig } = job.data;

    this.logger.log(`Processing export job ${executionId} for org ${organizationId}`);

    // Update status to processing
    await this.prisma.exportJob.update({
      where: { id: executionId },
      data: { status: 'PROCESSING', progress: 5 },
    });

    try {
      // 1. Load column configuration
      await job.updateProgress(10);
      const columns = await this.buildColumns(organizationId, columnConfig);

      // 2. Count total rows
      const totalRows = await this.countRows(organizationId, exportType, filters);
      await this.prisma.exportJob.update({
        where: { id: executionId },
        data: { totalRows },
      });

      // 3. Stream data and generate file
      await job.updateProgress(20);
      const dataStream = this.createDataStream(organizationId, exportType, filters, columns);

      let fileBuffer: Buffer;
      if (format === 'XLSX') {
        if (totalRows > 10000) {
          // Stream for large exports
          const excelStream = await this.excelExportService.streamExport(dataStream, columns);
          fileBuffer = await this.streamToBuffer(excelStream);
        } else {
          // Buffer for small exports
          const rows = await this.collectRows(dataStream);
          fileBuffer = await this.excelExportService.generateBuffer(rows, columns);
        }
      } else if (format === 'CSV') {
        fileBuffer = await this.generateCsv(dataStream, columns);
      } else {
        fileBuffer = await this.generateJson(dataStream);
      }

      await job.updateProgress(80);

      // 4. Upload to storage
      const fileExt = format.toLowerCase();
      const fileKey = `exports/${organizationId}/${executionId}.${fileExt}`;
      const mimeType = this.getMimeType(format);

      await this.storage.upload(fileKey, fileBuffer, mimeType);

      // 5. Generate signed URL
      const signedUrl = await this.storage.getSignedUrl(fileKey, 7 * 24 * 60 * 60); // 7 days

      // 6. Update job record
      await this.prisma.exportJob.update({
        where: { id: executionId },
        data: {
          status: 'COMPLETED',
          progress: 100,
          processedRows: totalRows,
          fileUrl: signedUrl,
          fileSizeBytes: fileBuffer.length,
          completedAt: new Date(),
          expiresAt: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000),
        },
      });

      this.logger.log(`Export job ${executionId} completed: ${totalRows} rows, ${fileBuffer.length} bytes`);
      return { fileKey, rowCount: totalRows };

    } catch (error) {
      this.logger.error(`Export job ${executionId} failed: ${error.message}`);
      await this.prisma.exportJob.update({
        where: { id: executionId },
        data: {
          status: 'FAILED',
          errorMessage: error.message,
          errorDetails: { stack: error.stack },
        },
      });
      throw error;
    }
  }

  @OnWorkerEvent('failed')
  onFailed(job: Job<ExportJobData>, error: Error) {
    this.logger.error(`Export job ${job.data.executionId} failed after retries: ${error.message}`);
  }

  private async buildColumns(orgId: string, config: ExportJobData['columnConfig']): Promise<ColumnDefinition[]> {
    const columns = [...this.flatFileService.getCoreColumns()];

    if (config.includeInvestigations) {
      columns.push(...this.flatFileService.getInvestigationColumns(config.maxInvestigations));
    }

    if (config.includeTaggedFields) {
      const tags = await this.flatFileService.getTaggedFields(orgId);
      columns.push(...this.flatFileService.getTaggedColumns(tags));
    }

    if (config.includeOverflow) {
      columns.push(
        { field: 'all_custom_fields', label: 'All Custom Fields', type: 'string' },
        { field: 'all_investigations', label: 'All Investigations', type: 'string' },
      );
    }

    return columns;
  }

  private async *createDataStream(
    orgId: string,
    exportType: ExportType,
    filters: Record<string, unknown>,
    columns: ColumnDefinition[],
  ): AsyncIterable<Record<string, unknown>> {
    const batchSize = 1000;
    let offset = 0;

    while (true) {
      const cases = await this.prisma.case.findMany({
        where: { organizationId: orgId, ...this.buildWhereClause(filters) },
        include: {
          category: true,
          assignee: true,
          location: true,
          businessUnit: true,
          investigations: { take: 3 },
          // ... other includes
        },
        skip: offset,
        take: batchSize,
        orderBy: { createdAt: 'desc' },
      });

      if (cases.length === 0) break;

      for (const case_ of cases) {
        yield this.denormalizeCase(case_, columns);
      }

      offset += batchSize;
    }
  }

  private denormalizeCase(case_: CaseWithRelations, columns: ColumnDefinition[]): Record<string, unknown> {
    // Map case to flat row matching column definitions
  }

  private getMimeType(format: ExportFormat): string {
    switch (format) {
      case 'XLSX': return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet';
      case 'CSV': return 'text/csv';
      case 'JSON': return 'application/json';
      default: return 'application/octet-stream';
    }
  }
}
```
  </action>
  <verify>npm run lint passes for the processor file</verify>
  <done>FlatExportProcessor handles async export jobs with progress tracking and streaming support</done>
</task>

<task type="auto">
  <name>Task 3: Create ExportsController</name>
  <files>apps/backend/src/modules/analytics/exports/exports.controller.ts</files>
  <action>
Create REST controller for export endpoints:

```typescript
@Controller('api/v1/exports')
@UseGuards(JwtAuthGuard)
@ApiTags('exports')
export class ExportsController {
  constructor(
    private flatFileService: FlatFileService,
    private prisma: PrismaService,
    @InjectQueue(EXPORT_QUEUE_NAME) private exportQueue: Queue,
  ) {}

  // Tag management
  @Post('tags')
  @Roles(Role.SYSTEM_ADMIN, Role.COMPLIANCE_OFFICER)
  @ApiOperation({ summary: 'Configure tagged export fields' })
  async configureTags(
    @CurrentUser() user: User,
    @Body() dto: CreateTagDto[],
  ): Promise<ReportFieldTag[]> {
    return this.flatFileService.configureTag(user.organizationId, user.id, dto);
  }

  @Get('tags')
  @ApiOperation({ summary: 'List configured tags' })
  async getTags(@CurrentUser() user: User): Promise<ReportFieldTag[]> {
    return this.flatFileService.getTaggedFields(user.organizationId);
  }

  @Delete('tags/:slot')
  @Roles(Role.SYSTEM_ADMIN, Role.COMPLIANCE_OFFICER)
  async removeTag(@CurrentUser() user: User, @Param('slot', ParseIntPipe) slot: number): Promise<void> {
    return this.flatFileService.removeTag(user.organizationId, slot);
  }

  @Get('tags/preview')
  @ApiOperation({ summary: 'Preview tags with sample case' })
  async previewTags(
    @CurrentUser() user: User,
    @Query('caseId') caseId?: string,
  ): Promise<TagPreview[]> {
    return this.flatFileService.previewTag(user.organizationId, caseId);
  }

  // Export jobs
  @Post('flat-file')
  @ApiOperation({ summary: 'Create flat file export job' })
  @HttpCode(HttpStatus.ACCEPTED)
  async createFlatExport(
    @CurrentUser() user: User,
    @Body() dto: CreateExportJobDto,
  ): Promise<ExportJobResponseDto> {
    // Create job record
    const job = await this.flatFileService.createExportJob(user.organizationId, user.id, dto);

    // Queue for async processing
    await this.exportQueue.add('flat-export', {
      executionId: job.id,
      organizationId: user.organizationId,
      exportType: dto.exportType,
      format: dto.format,
      filters: dto,
      columnConfig: {
        includeInvestigations: dto.includeInvestigations,
        maxInvestigations: dto.maxInvestigations,
        includeTaggedFields: dto.includeTaggedFields,
        includeOverflow: dto.includeOverflow,
      },
      createdById: user.id,
    }, {
      attempts: 3,
      backoff: { type: 'exponential', delay: 5000 },
    });

    return {
      jobId: job.id,
      status: job.status,
      progress: 0,
      format: job.format,
      createdAt: job.createdAt,
    };
  }

  @Get()
  @ApiOperation({ summary: 'List export jobs' })
  async listExports(
    @CurrentUser() user: User,
    @Query() query: PaginationDto,
  ): Promise<PaginatedResult<ExportJobResponseDto>> {
    return this.flatFileService.listExportJobs(user.organizationId, user.id, query);
  }

  @Get(':id')
  @ApiOperation({ summary: 'Get export job status' })
  async getExport(
    @CurrentUser() user: User,
    @Param('id') id: string,
  ): Promise<ExportJobResponseDto> {
    const job = await this.flatFileService.getExportJob(user.organizationId, id);
    return this.mapToResponse(job);
  }

  @Get(':id/download')
  @ApiOperation({ summary: 'Get download URL' })
  async downloadExport(
    @CurrentUser() user: User,
    @Param('id') id: string,
  ): Promise<{ url: string; expiresAt: Date }> {
    const job = await this.flatFileService.getExportJob(user.organizationId, id);

    if (job.status !== 'COMPLETED') {
      throw new BadRequestException('Export not ready for download');
    }

    return {
      url: job.fileUrl,
      expiresAt: job.expiresAt,
    };
  }

  @Delete(':id')
  @ApiOperation({ summary: 'Cancel pending export' })
  async cancelExport(
    @CurrentUser() user: User,
    @Param('id') id: string,
  ): Promise<void> {
    return this.flatFileService.cancelExportJob(user.organizationId, id);
  }

  private mapToResponse(job: ExportJob): ExportJobResponseDto {
    return {
      jobId: job.id,
      status: job.status,
      progress: job.progress,
      totalRows: job.totalRows,
      format: job.format,
      fileSizeBytes: job.fileSizeBytes,
      downloadUrl: job.status === 'COMPLETED' ? `/api/v1/exports/${job.id}/download` : undefined,
      expiresAt: job.expiresAt,
      createdAt: job.createdAt,
      completedAt: job.completedAt,
      errorMessage: job.errorMessage,
    };
  }
}
```
  </action>
  <verify>npm run lint && npm run typecheck passes</verify>
  <done>ExportsController exposes REST endpoints for tags and export job management</done>
</task>

</tasks>

<verification>
```bash
cd apps/backend
npm run lint -- --fix
npm run typecheck
```
</verification>

<success_criteria>
- ExcelExportService supports streaming for large exports (>10k rows)
- FlatExportProcessor executes async jobs with progress tracking
- ExportsController exposes tag management and job endpoints
- Files uploaded to Azure Blob with 7-day expiration
- Export jobs have proper retry logic (3 attempts, exponential backoff)
</success_criteria>

<output>
After completion, create `.planning/phases/11-analytics-reporting/11-07-SUMMARY.md`
</output>
